{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost: Fit/Predict\n",
    "It's time to create your first XGBoost model! As Sergey showed you in the video, you can use the scikit-learn .fit() / .predict() paradigm that you are already familiar to build your XGBoost models, as the xgboost library has a scikit-learn compatible API!\n",
    "\n",
    "Here, you'll be working with churn data. This dataset contains imaginary data from a ride-sharing app with user behaviors over their first month of app usage in a set of imaginary cities as well as whether they used the service 5 months after sign-up. It has been pre-loaded for you into a DataFrame called churn_data - explore it in the Shell!\n",
    "\n",
    "Your goal is to use the first month's worth of data to predict whether the app's users will remain users of the service at the 5 month mark. This is a typical setup for a churn prediction problem. To do this, you'll split the data into training and test sets, fit a small xgboost model on the training set, and evaluate its performance on the test set by computing its accuracy.\n",
    "\n",
    "pandas and numpy have been imported as pd and np, and train_test_split has been imported from sklearn.model_selection. Additionally, the arrays for the features and the target have been created as X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data = pd.read_csv('datasets/churn_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45236 entries, 0 to 45235\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   ID                 45236 non-null  int64 \n",
      " 1   title              45236 non-null  object\n",
      " 2   paymentMethod      45236 non-null  object\n",
      " 3   couponDiscount     45236 non-null  int64 \n",
      " 4   purchaseValue      45236 non-null  int64 \n",
      " 5   giftwrapping       45236 non-null  int64 \n",
      " 6   throughAffiliate   45236 non-null  int64 \n",
      " 7   shippingFees       45236 non-null  int64 \n",
      " 8   dvd                45236 non-null  int64 \n",
      " 9   blueray            45236 non-null  int64 \n",
      " 10  vinyl              45236 non-null  int64 \n",
      " 11  videogame          45236 non-null  int64 \n",
      " 12  videogameDownload  45236 non-null  int64 \n",
      " 13  tvEquiment         45236 non-null  int64 \n",
      " 14  prodOthers         45236 non-null  int64 \n",
      " 15  prodSecondHand     45236 non-null  int64 \n",
      " 16  returnCustomer     45236 non-null  int64 \n",
      "dtypes: int64(15), object(2)\n",
      "memory usage: 5.9+ MB\n"
     ]
    }
   ],
   "source": [
    "churn_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                   0\n",
       "title                0\n",
       "paymentMethod        0\n",
       "couponDiscount       0\n",
       "purchaseValue        0\n",
       "giftwrapping         0\n",
       "throughAffiliate     0\n",
       "shippingFees         0\n",
       "dvd                  0\n",
       "blueray              0\n",
       "vinyl                0\n",
       "videogame            0\n",
       "videogameDownload    0\n",
       "tvEquiment           0\n",
       "prodOthers           0\n",
       "prodSecondHand       0\n",
       "returnCustomer       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = {'Mr':1, 'Mrs':2, 'Others':3, 'Company':4}\n",
    "payment = {'Invoice':1, 'Cash':2, 'Current Account':3, 'Credit Card':4}\n",
    "\n",
    "\n",
    "churn_data['paymentMethod'] = churn_data['paymentMethod'].map(payment)\n",
    "churn_data['title'] = churn_data['title'].map(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>paymentMethod</th>\n",
       "      <th>couponDiscount</th>\n",
       "      <th>purchaseValue</th>\n",
       "      <th>giftwrapping</th>\n",
       "      <th>throughAffiliate</th>\n",
       "      <th>shippingFees</th>\n",
       "      <th>dvd</th>\n",
       "      <th>blueray</th>\n",
       "      <th>vinyl</th>\n",
       "      <th>videogame</th>\n",
       "      <th>videogameDownload</th>\n",
       "      <th>tvEquiment</th>\n",
       "      <th>prodOthers</th>\n",
       "      <th>prodSecondHand</th>\n",
       "      <th>returnCustomer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  title  paymentMethod  couponDiscount  purchaseValue  giftwrapping  \\\n",
       "0   1      1              3               1              2             0   \n",
       "1   3      1              1               0              1             0   \n",
       "2   5      1              2               0              4             0   \n",
       "3   7      1              2               0              4             0   \n",
       "4   8      1              2               0              4             0   \n",
       "\n",
       "   throughAffiliate  shippingFees  dvd  blueray  vinyl  videogame  \\\n",
       "0                 1             0    0        1      0          0   \n",
       "1                 0             1    0        0      0          0   \n",
       "2                 0             0    0        0      1          0   \n",
       "3                 1             0    1        0      0          0   \n",
       "4                 1             0    2        0      0          0   \n",
       "\n",
       "   videogameDownload  tvEquiment  prodOthers  prodSecondHand  returnCustomer  \n",
       "0                  0           0           0               0               0  \n",
       "1                  0           0           0               0               0  \n",
       "2                  0           0           0               0               0  \n",
       "3                  0           0           0               0               0  \n",
       "4                  0           0           0               0               0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.817418\n"
     ]
    }
   ],
   "source": [
    "# Import xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,1:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees\n",
    "Your task in this exercise is to make a simple decision tree using scikit-learn's DecisionTreeClassifier on the breast cancer dataset that comes pre-loaded with scikit-learn.\n",
    "\n",
    "This dataset contains numeric measurements of various dimensions of individual tumors (such as perimeter and texture) from breast biopsies and a single outcome value (the tumor is either malignant, or benign).\n",
    "\n",
    "We've preloaded the dataset of samples (measurements) into X and the target values per tumor into y. Now, you have to split the complete dataset into training and testing sets, and then train a DecisionTreeClassifier. You'll specify a parameter called max_depth. Many other parameters can be modified within this model, and you can check all of them out [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
    "'symmetry_worst', 'fractal_dimension_worst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = pd.read_csv('datasets/breast-cancer-wisconsin-data.csv', usecols=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis = {'M':1, 'B':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer_data.iloc[:, 1:]\n",
    "y = cancer_data['diagnosis'].map(diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring accuracy\n",
    "You'll now practice using XGBoost's learning API through its baked in cross-validation capabilities. As Sergey discussed in the previous video, XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a DMatrix.\n",
    "\n",
    "In the previous exercise, the input datasets were converted into DMatrix data on the fly, but when you use the xgboost cv object, you have to first explicitly convert your data into a DMatrix. So, that's what you will do here before running cross-validation on churn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-error-mean  train-error-std  test-error-mean  test-error-std\n",
      "0          0.182863         0.000298         0.182863        0.000596\n",
      "1          0.182863         0.000298         0.182863        0.000596\n",
      "2          0.182863         0.000298         0.182863        0.000596\n",
      "3          0.182863         0.000298         0.182863        0.000596\n",
      "4          0.182863         0.000298         0.182863        0.000596\n",
      "0.817137\n"
     ]
    }
   ],
   "source": [
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the DMatrix from X and y: churn_dmatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                  nfold=3, num_boost_round=5, \n",
    "                  metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring AUC\n",
    "Now that you've used cross-validation to compute average out-of-sample accuracy (after converting from an error), it's very easy to compute any other metric you might be interested in. All you have to do is pass it (or a list of metrics) in as an argument to the metrics parameter of xgb.cv().\n",
    "\n",
    "Your job in this exercise is to compute another common metric used in binary classification - the area under the curve (\"auc\"). As before, churn_data is available in your workspace, along with the DMatrix churn_dmatrix and parameter dictionary params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
      "0        0.581231       0.001747       0.575866      0.003559\n",
      "1        0.595397       0.000886       0.591900      0.001501\n",
      "2        0.598093       0.001619       0.593891      0.001603\n",
      "3        0.600262       0.001323       0.596004      0.001414\n",
      "4        0.605720       0.002166       0.600354      0.001781\n",
      "0.6003539999999999\n"
     ]
    }
   ],
   "source": [
    "# Perform cross_validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                  nfold=3, num_boost_round=5, \n",
    "                  metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees as base learners\n",
    "It's now time to build an XGBoost model to predict house prices - not in Boston, Massachusetts, as you saw in the video, but in Ames, Iowa! This dataset of housing prices has been pre-loaded into a DataFrame called df. If you explore it in the Shell, you'll see that there are a variety of features about the house and its location in the city.\n",
    "\n",
    "In this exercise, your goal is to use trees as base learners. By default, XGBoost uses trees as base learners, so you don't have to specify that you want to use trees here with booster=\"gbtree\".\n",
    "\n",
    "xgboost has been imported as xgb and the arrays for the features and the target are available in X and y, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "housingPrice = pd.read_csv('datasets/ames_housing_trimmed_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housingPrice.iloc[:, :-1]\n",
    "y = housingPrice.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Remodeled</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>...</th>\n",
       "      <th>BldgType_TwnhsE</th>\n",
       "      <th>HouseStyle_1.5Unf</th>\n",
       "      <th>HouseStyle_1Story</th>\n",
       "      <th>HouseStyle_2.5Fin</th>\n",
       "      <th>HouseStyle_2.5Unf</th>\n",
       "      <th>HouseStyle_2Story</th>\n",
       "      <th>HouseStyle_SFoyer</th>\n",
       "      <th>HouseStyle_SLvl</th>\n",
       "      <th>PavedDrive_P</th>\n",
       "      <th>PavedDrive_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>1710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>0</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>1786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>2198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>60</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>1647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>20</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1978</td>\n",
       "      <td>1</td>\n",
       "      <td>2073</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>70</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1941</td>\n",
       "      <td>1</td>\n",
       "      <td>2340</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>20</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1950</td>\n",
       "      <td>1</td>\n",
       "      <td>1078</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>20</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>1256</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0             60         65.0     8450            7            5       2003   \n",
       "1             20         80.0     9600            6            8       1976   \n",
       "2             60         68.0    11250            7            5       2001   \n",
       "3             70         60.0     9550            7            5       1915   \n",
       "4             60         84.0    14260            8            5       2000   \n",
       "...          ...          ...      ...          ...          ...        ...   \n",
       "1455          60         62.0     7917            6            5       1999   \n",
       "1456          20         85.0    13175            6            6       1978   \n",
       "1457          70         66.0     9042            7            9       1941   \n",
       "1458          20         68.0     9717            5            6       1950   \n",
       "1459          20         75.0     9937            5            6       1965   \n",
       "\n",
       "      Remodeled  GrLivArea  BsmtFullBath  BsmtHalfBath  ...  BldgType_TwnhsE  \\\n",
       "0             0       1710             1             0  ...                0   \n",
       "1             0       1262             0             1  ...                0   \n",
       "2             1       1786             1             0  ...                0   \n",
       "3             1       1717             1             0  ...                0   \n",
       "4             0       2198             1             0  ...                0   \n",
       "...         ...        ...           ...           ...  ...              ...   \n",
       "1455          1       1647             0             0  ...                0   \n",
       "1456          1       2073             1             0  ...                0   \n",
       "1457          1       2340             0             0  ...                0   \n",
       "1458          1       1078             1             0  ...                0   \n",
       "1459          0       1256             1             0  ...                0   \n",
       "\n",
       "      HouseStyle_1.5Unf  HouseStyle_1Story  HouseStyle_2.5Fin  \\\n",
       "0                     0                  0                  0   \n",
       "1                     0                  1                  0   \n",
       "2                     0                  0                  0   \n",
       "3                     0                  0                  0   \n",
       "4                     0                  0                  0   \n",
       "...                 ...                ...                ...   \n",
       "1455                  0                  0                  0   \n",
       "1456                  0                  1                  0   \n",
       "1457                  0                  0                  0   \n",
       "1458                  0                  1                  0   \n",
       "1459                  0                  1                  0   \n",
       "\n",
       "      HouseStyle_2.5Unf  HouseStyle_2Story  HouseStyle_SFoyer  \\\n",
       "0                     0                  1                  0   \n",
       "1                     0                  0                  0   \n",
       "2                     0                  1                  0   \n",
       "3                     0                  1                  0   \n",
       "4                     0                  1                  0   \n",
       "...                 ...                ...                ...   \n",
       "1455                  0                  1                  0   \n",
       "1456                  0                  0                  0   \n",
       "1457                  0                  1                  0   \n",
       "1458                  0                  0                  0   \n",
       "1459                  0                  0                  0   \n",
       "\n",
       "      HouseStyle_SLvl  PavedDrive_P  PavedDrive_Y  \n",
       "0                   0             0             1  \n",
       "1                   0             0             1  \n",
       "2                   0             0             1  \n",
       "3                   0             0             1  \n",
       "4                   0             0             1  \n",
       "...               ...           ...           ...  \n",
       "1455                0             0             1  \n",
       "1456                0             0             1  \n",
       "1457                0             0             1  \n",
       "1458                0             0             1  \n",
       "1459                0             0             1  \n",
       "\n",
       "[1460 rows x 56 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 26483.951421\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimator=10, booster='gbtree', seed=123)\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear base learners\n",
    "Now that you've used trees as base models in XGBoost, let's use the other kind of base model that can be used with XGBoost - a linear learner. This model, although not as commonly used in XGBoost, allows you to create a regularized linear regression using XGBoost's powerful learning API. However, because it's uncommon, you have to use XGBoost's own non-scikit-learn compatible functions to build the model, such as xgb.train().\n",
    "\n",
    "In order to do this you must create the parameter dictionary that describes the kind of booster you want to use (similarly to how you created the dictionary in Chapter 1 when you used xgb.cv()). The key-value pair that defines the booster type (base model) you need is \"booster\":\"gblinear\".\n",
    "\n",
    "Once you've created the model, you can use the .train() and .predict() methods of the model just like you've done in the past.\n",
    "\n",
    "Here, the data has already been split into training and testing sets, so you can dive right into creating the DMatrix objects required by the XGBoost learning API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "RMSE: 47518.563687\n"
     ]
    }
   ],
   "source": [
    "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "DM_train = xgb.DMatrix(X_train, y_train)\n",
    "DM_test =  xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=DM_train, num_boost_round=5)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model quality\n",
    "It's now time to begin evaluating model quality.\n",
    "\n",
    "Here, you will compare the RMSE and MAE of a cross-validated XGBoost model on the Ames housing data. As in previous exercises, all necessary modules have been pre-loaded and the data is available in the DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0    141767.535156      429.449158   142980.437500    1193.795676\n",
      "1    102832.544922      322.470330   104891.398438    1223.156387\n",
      "2     75872.613281      266.473439    79478.939453    1601.346707\n",
      "3     57245.652343      273.623448    62411.921875    2220.149857\n",
      "4     44401.298828      316.423666    51348.278320    2963.378163\n",
      "4    51348.27832\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics='rmse', as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "   train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n",
      "0   127343.462890     668.310785  127633.994140   2404.004942\n",
      "1    89770.062500     456.955637   90122.501953   2107.910521\n",
      "2    63580.789062     263.403775   64278.558594   1887.566667\n",
      "3    45633.156250     151.884193   46819.169922   1459.817248\n",
      "4    33587.091797      86.999063   35670.646485   1140.609795\n",
      "4    35670.646485\n",
      "Name: test-mae-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics='mae', as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-mae-mean\"]).tail(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using regularization in XGBoost\n",
    "Having seen an example of l1 regularization in the video, you'll now vary the l2 regularization penalty - also known as \"lambda\" - and see its effect on overall model performance on the Ames housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Best rmse as a function of l2:\n",
      "    l2          rmse\n",
      "0    1  52275.355468\n",
      "1   10  57746.064453\n",
      "2  100  76624.625000\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "reg_params = [1, 10, 100]\n",
    "\n",
    "# Create the initial parameter dictionary for varying l2 strength: params\n",
    "params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create an empty list for storing rmses as a function of l2 complexity\n",
    "rmses_l2 = []\n",
    "\n",
    "# Iterate over reg_params\n",
    "for reg in reg_params:\n",
    "\n",
    "    # Update l2 strength\n",
    "    params[\"lambda\"] = reg\n",
    "    \n",
    "    # Pass this updated param dictionary into cv\n",
    "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append best rmse (final round) to rmses_l2\n",
    "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
    "\n",
    "# Look at best rmse per l2 param\n",
    "print(\"Best rmse as a function of l2:\")\n",
    "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\", \"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing individual XGBoost trees\n",
    "Now that you've used XGBoost to both build and evaluate regression as well as classification models, you should get a handle on how to visually explore your models. Here, you will visualize individual trees from the fully boosted model that XGBoost creates using the entire housing dataset.\n",
    "\n",
    "XGBoost has a plot_tree() function that makes this type of visualization easy. Once you train a model using the XGBoost learning API, you can pass it to the plot_tree() function along with the number of trees you want to plot using the num_trees argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":2}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the first tree\n",
    "# xgb.plot_tree(xg_reg, num_trees=0)\n",
    "# plt.show()\n",
    "\n",
    "# Plot the fifth tree\n",
    "# xgb.plot_tree(xg_reg, num_trees=4)\n",
    "# plt.show()\n",
    "\n",
    "# Plot the last tree sideways\n",
    "# xgb.plot_tree(xg_reg, num_trees=9, rankdir='LR')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing feature importances: What features are most important in my dataset\n",
    "Another way to visualize your XGBoost models is to examine the importance of each feature column in the original dataset within the model.\n",
    "\n",
    "One simple way of doing this involves counting the number of times each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost has a plot_importance() function that allows you to do exactly this, and you'll get a chance to use it in this exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:11:40] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {'objective':'reg:linear',\n",
    "         'max_depth':4}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(dtrain=housing_dmatrix, params=params, num_boost_round=10)\n",
    "\n",
    "\n",
    "# Plot the feature importances\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the number of boosting rounds\n",
    "Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You'll use xgb.cv() inside a for loop and build one model per num_boost_round parameter.\n",
    "\n",
    "Here, you'll continue working with the Ames housing dataset. The features are available in the array X, and the target vector is contained in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boosting_rounds          rmse\n",
      "0                    5  50903.299479\n",
      "1                   10  34774.194010\n",
      "2                   15  32895.097656\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated boosting round selection using early_stopping\n",
    "Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within xgb.cv(). This is done using a technique called early stopping.\n",
    "\n",
    "Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds. Here you will use the early_stopping_rounds parameter in xgb.cv() with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when num_boost_rounds is reached, then early stopping does not occur.\n",
    "\n",
    "Here, the DMatrix and parameter dictionary have been created for you. Your task is to use cross-validation with early stopping. Go for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0     141871.635417      403.636200   142640.656250     705.565422\n",
      "1     103057.036458       73.769561   104907.664063     111.119966\n",
      "2      75975.971354      253.728779    79262.059895     563.766991\n",
      "3      57420.531250      521.658363    61620.138021    1087.693432\n",
      "4      44552.955729      544.169200    50437.561198    1846.446330\n",
      "5      35763.949219      681.795751    43035.662760    2034.471024\n",
      "6      29861.464193      769.571238    38600.881510    2169.798065\n",
      "7      25994.676432      756.521548    36071.817708    2109.795430\n",
      "8      23306.835286      759.237345    34383.184896    1934.546688\n",
      "9      21459.769531      745.624998    33509.140625    1887.374937\n",
      "10     20148.720703      749.612103    32916.808594    1850.894249\n",
      "11     19215.382162      641.387014    32197.832682    1734.456935\n",
      "12     18627.389323      716.256596    31770.854167    1802.154652\n",
      "13     17960.695312      557.043568    31482.781901    1779.124534\n",
      "14     17559.736979      631.412969    31389.989583    1892.319887\n",
      "15     17205.713542      590.171791    31302.882813    1955.165071\n",
      "16     16876.571940      703.631755    31234.058594    1880.705796\n",
      "17     16597.662109      703.677683    31318.347656    1828.860164\n",
      "18     16330.461263      607.274034    31323.634766    1775.910706\n",
      "19     16005.973308      520.470457    31204.136719    1739.077370\n",
      "20     15814.299805      518.604759    31089.862630    1756.021674\n",
      "21     15493.405599      505.617080    31047.998047    1624.672407\n",
      "22     15270.734375      502.018453    31056.915364    1668.043691\n",
      "23     15086.382487      503.912747    31024.984375    1548.984461\n",
      "24     14917.608073      486.205906    30983.684245    1663.130820\n",
      "25     14709.589518      449.668010    30989.476563    1686.667469\n",
      "26     14457.286784      376.788126    30952.113932    1613.173861\n",
      "27     14185.566732      383.103149    31066.901042    1648.533407\n",
      "28     13934.066406      473.465184    31095.642578    1709.226563\n",
      "29     13749.644857      473.670302    31103.887370    1778.880069\n",
      "30     13549.836589      454.898052    30976.085287    1744.514533\n",
      "31     13413.484700      399.603323    30938.468750    1746.053513\n",
      "32     13275.915690      415.408663    30931.001302    1772.468988\n",
      "33     13085.877930      493.793211    30929.056641    1765.541578\n",
      "34     12947.181641      517.790106    30890.628906    1786.511895\n",
      "35     12846.027669      547.732724    30884.493490    1769.729215\n",
      "36     12702.378581      505.523315    30833.542969    1691.002142\n",
      "37     12532.244466      508.298594    30856.688151    1771.445059\n",
      "38     12384.055338      536.225403    30818.016927    1782.784630\n",
      "39     12198.443685      545.165622    30839.393229    1847.327529\n",
      "40     12054.583984      508.842133    30776.966797    1912.779587\n",
      "41     11897.036784      477.177937    30794.702474    1919.674832\n",
      "42     11756.222005      502.992373    30780.957031    1906.820066\n",
      "43     11618.847005      519.837501    30783.755859    1951.259331\n",
      "44     11484.080078      578.429042    30776.731120    1953.447693\n",
      "45     11356.552734      565.368794    30758.543620    1947.454953\n",
      "46     11193.558268      552.298848    30729.972005    1985.699338\n",
      "47     11071.315429      604.089960    30732.664062    1966.998275\n",
      "48     10950.778320      574.863283    30712.241536    1957.751573\n",
      "49     10824.865885      576.665756    30720.854167    1950.512441\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params,nfold=3,early_stopping_rounds=10, \n",
    "                    metrics='rmse', seed=123, num_boost_round=50, as_pandas=True)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning eta\n",
    "It's time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You'll begin by tuning the \"eta\", also known as the learning rate.\n",
    "\n",
    "The learning rate in XGBoost is a parameter that can range between 0 and 1, with higher values of \"eta\" penalizing feature weights more strongly, causing much stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "     eta      best_rmse\n",
      "0  0.001  195736.406250\n",
      "1  0.010  179932.192708\n",
      "2  0.100   79759.411458\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, .01, .1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, early_stopping_rounds=5, num_boost_round=10, metrics='rmse', seed=123, as_pandas=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_depth\n",
    "In this exercise, your job is to tune max_depth, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:41] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "   max_depth     best_rmse\n",
      "0          2  37957.468750\n",
      "1          5  35596.601562\n",
      "2         10  36065.546875\n",
      "3         20  36739.578125\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:linear\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2, 5, 10, 20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, early_stopping_rounds=5, num_boost_round=10, metrics='rmse', seed=123, as_pandas=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning colsample_bytree\n",
    "Now, it's time to tune \"colsample_bytree\". You've already seen this if you've ever worked with scikit-learn's RandomForestClassifier or RandomForestRegressor, where it just was called max_features. In both xgboost and sklearn, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, colsample_bytree must be specified as a float between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:11:42] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:42] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:42] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:42] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:42] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:42] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:42] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[15:11:42] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "   colsample_bytree     best_rmse\n",
      "0               0.1  48193.447265\n",
      "1               0.5  36013.544922\n",
      "2               0.8  35932.962891\n",
      "3               1.0  35836.044922\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values\n",
    "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params[\"colsample_bytree\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search with XGBoost\n",
    "Now that you've learned how to tune parameters individually with XGBoost, let's take your parameter tuning to the next level by using scikit-learn's GridSearch and RandomizedSearch capabilities with internal cross-validation using the GridSearchCV and RandomizedSearchCV functions. You will use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously. Let's get to work, starting with GridSearchCV!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'colsample_bytree': 0.7, 'max_depth': 5, 'n_estimators': 50}\n",
      "Lowest RMSE found:  30744.105707685176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [.3, .7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(param_grid=gbm_param_grid, estimator=gbm, scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search with XGBoost\n",
    "Often, GridSearchCV can be really time consuming, so in practice, you may want to use RandomizedSearchCV instead, as you will do in this exercise. The good news is you only have to make a few modifications to your GridSearchCV code to do RandomizedSearchCV. The key difference is you have to specify a param_distributions parameter instead of a param_grid parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'n_estimators': 25, 'max_depth': 4}\n",
      "Lowest RMSE found:  29998.4522530019\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid \n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(param_distributions=gbm_param_grid, estimator=gbm, scoring='neg_mean_squared_error', n_iter=5, cv=4)\n",
    "\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical columns I: LabelEncoder\n",
    "Now that you've seen what will need to be done to get the housing data ready for XGBoost, let's go through the process step-by-step.\n",
    "\n",
    "First, you will need to fill in missing values - as you saw previously, the column LotFrontage has many missing values. Then, you will need to encode any categorical columns in the dataset using one-hot encoding so that they are encoded numerically. You can watch this video from Supervised Learning with scikit-learn for a refresher on the idea.\n",
    "\n",
    "The data has five categorical columns: MSZoning, PavedDrive, Neighborhood, BldgType, and HouseStyle. Scikit-learn has a LabelEncoder function that converts the values in each categorical column into integers. You'll practice using this here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/ames_unprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSZoning Neighborhood BldgType HouseStyle PavedDrive\n",
      "0       RL      CollgCr     1Fam     2Story          Y\n",
      "1       RL      Veenker     1Fam     1Story          Y\n",
      "2       RL      CollgCr     1Fam     2Story          Y\n",
      "3       RL      Crawfor     1Fam     2Story          Y\n",
      "4       RL      NoRidge     1Fam     2Story          Y\n",
      "   MSZoning  Neighborhood  BldgType  HouseStyle  PavedDrive\n",
      "0         3             5         0           5           2\n",
      "1         3            24         0           2           2\n",
      "2         3             5         0           5           2\n",
      "3         3             6         0           5           2\n",
      "4         3            15         0           5           2\n"
     ]
    }
   ],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fill missing values with 0\n",
    "df.LotFrontage = df['LotFrontage'].fillna(0)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_mask = (df.dtypes == 'object')\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = df.columns[categorical_mask].tolist()\n",
    "\n",
    "# Print the head of the categorical columns\n",
    "print(df[categorical_columns].head())\n",
    "\n",
    "# Create LabelEncoder object: le\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to categorical columns\n",
    "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
    "\n",
    "# Print the head of the LabelEncoded categorical columns\n",
    "print(df[categorical_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical columns II: OneHotEncoder\n",
    "Okay - so you have your categorical columns encoded numerically. Can you now move onto using pipelines and XGBoost? Not yet! In the categorical columns of this dataset, there is no natural ordering between the entries. As an example: Using LabelEncoder, the CollgCr Neighborhood was encoded as 5, while the Veenker Neighborhood was encoded as 24, and Crawfor as 6. Is Veenker \"greater\" than Crawfor and CollgCr? No - and allowing the model to assume this natural ordering may result in poor performance.\n",
    "\n",
    "As a result, there is another step needed: You have to apply a one-hot encoding to create binary, or \"dummy\" variables. You can do this using scikit-learn's OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_mask = ['MSZoning', 'Neighborhood', 'BldgType', 'HouseStyle', 'PavedDrive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>HouseStyle</th>\n",
       "      <th>PavedDrive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSZoning  Neighborhood  BldgType  HouseStyle  PavedDrive\n",
       "0            3             5         0           5           2\n",
       "1            3            24         0           2           2\n",
       "2            3             5         0           5           2\n",
       "3            3             6         0           5           2\n",
       "4            3            15         0           5           2\n",
       "...        ...           ...       ...         ...         ...\n",
       "1455         3             8         0           5           2\n",
       "1456         3            14         0           2           2\n",
       "1457         3             6         0           5           2\n",
       "1458         3            12         0           2           2\n",
       "1459         3             7         0           2           2\n",
       "\n",
       "[1460 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, categorical_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/ames_unprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Import OneHotEncoder\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Create OneHotEncoder: ohe\n",
    "# ohe = OneHotEncoder(categorical_columns, sparse=False)\n",
    "# df.LotFrontage = df['LotFrontage'].fillna(0)\n",
    "# # Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
    "# df_encoded = ohe.fit_transform(df)\n",
    "\n",
    "# # Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
    "# print(df_encoded[:5, :])\n",
    "\n",
    "# # Print the shape of the original DataFrame\n",
    "# print(df.shape)\n",
    "\n",
    "# # Print the shape of the transformed array\n",
    "# print(df_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical columns III: DictVectorizer\n",
    "Alright, one final trick before you dive into pipelines. The two step process you just went through - LabelEncoder followed by OneHotEncoder - can be simplified by using a DictVectorizer.\n",
    "\n",
    "Using a DictVectorizer on a DataFrame that has been converted to a dictionary allows you to get label encoding as well as one-hot encoding in one go.\n",
    "\n",
    "Your task is to work through this strategy in this exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/ames_unprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 2.000e+00 5.480e+02 1.710e+03 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
      "  8.450e+03 6.500e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 2.085e+05 2.003e+03]\n",
      " [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  1.000e+00 1.000e+00 2.000e+00 4.600e+02 1.262e+03 0.000e+00 0.000e+00\n",
      "  0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  9.600e+03 8.000e+01 2.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 8.000e+00 6.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 1.815e+05 1.976e+03]\n",
      " [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 1.000e+00 2.000e+00 6.080e+02 1.786e+03 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
      "  1.125e+04 6.800e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 1.000e+00 2.235e+05 2.001e+03]\n",
      " [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 1.000e+00 1.000e+00 6.420e+02 1.717e+03 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
      "  9.550e+03 6.000e+01 7.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 1.000e+00 1.400e+05 1.915e+03]\n",
      " [4.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 1.000e+00 2.000e+00 8.360e+02 2.198e+03 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
      "  1.426e+04 8.400e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 8.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 2.500e+05 2.000e+03]]\n",
      "{'MSSubClass': 23, 'MSZoning=RL': 27, 'LotFrontage': 22, 'LotArea': 21, 'Neighborhood=CollgCr': 34, 'BldgType=1Fam': 1, 'HouseStyle=2Story': 18, 'OverallQual': 55, 'OverallCond': 54, 'YearBuilt': 61, 'Remodeled': 59, 'GrLivArea': 11, 'BsmtFullBath': 6, 'BsmtHalfBath': 7, 'FullBath': 9, 'HalfBath': 12, 'BedroomAbvGr': 0, 'Fireplaces': 8, 'GarageArea': 10, 'PavedDrive=Y': 58, 'SalePrice': 60, 'Neighborhood=Veenker': 53, 'HouseStyle=1Story': 15, 'Neighborhood=Crawfor': 35, 'Neighborhood=NoRidge': 44, 'Neighborhood=Mitchel': 40, 'HouseStyle=1.5Fin': 13, 'Neighborhood=Somerst': 50, 'Neighborhood=NWAmes': 43, 'MSZoning=RM': 28, 'Neighborhood=OldTown': 46, 'Neighborhood=BrkSide': 32, 'BldgType=2fmCon': 2, 'HouseStyle=1.5Unf': 14, 'Neighborhood=Sawyer': 48, 'Neighborhood=NridgHt': 45, 'Neighborhood=NAmes': 41, 'BldgType=Duplex': 3, 'Neighborhood=SawyerW': 49, 'Neighborhood=IDOTRR': 38, 'PavedDrive=N': 56, 'Neighborhood=MeadowV': 39, 'BldgType=TwnhsE': 5, 'MSZoning=C (all)': 24, 'Neighborhood=Edwards': 36, 'Neighborhood=Timber': 52, 'PavedDrive=P': 57, 'HouseStyle=SFoyer': 19, 'MSZoning=FV': 25, 'Neighborhood=Gilbert': 37, 'HouseStyle=SLvl': 20, 'BldgType=Twnhs': 4, 'Neighborhood=StoneBr': 51, 'HouseStyle=2.5Unf': 17, 'Neighborhood=ClearCr': 33, 'Neighborhood=NPkVill': 42, 'HouseStyle=2.5Fin': 16, 'Neighborhood=Blmngtn': 29, 'Neighborhood=BrDale': 31, 'Neighborhood=SWISU': 47, 'MSZoning=RH': 26, 'Neighborhood=Blueste': 30}\n"
     ]
    }
   ],
   "source": [
    "# Import DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Convert df into a dictionary: df_dict\n",
    "df_dict = df.to_dict('records')\n",
    "\n",
    "# Create the DictVectorizer object: dv\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "# Apply dv on df: df_encoded\n",
    "df_encoded = dv.fit_transform(df_dict)\n",
    "\n",
    "# Print the resulting first five rows\n",
    "print(df_encoded[:5,:])\n",
    "\n",
    "# Print the vocabulary\n",
    "print(dv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing within a pipeline\n",
    "Now that you've seen what steps need to be taken individually to properly process the Ames housing data, let's use the much cleaner and more succinct DictVectorizer approach and put it alongside an XGBoostRegressor inside of a scikit-learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('ohe_onestep',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=False)),\n",
       "                ('xgb_model',\n",
       "                 XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "                              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "                              gpu_id=-1, importance_type='gain',\n",
       "                              interaction_constraints=None,\n",
       "                              learning_rate=0.300000012, max_delta_step=0,\n",
       "                              max_depth=6, min_child_weight=1, missing=nan,\n",
       "                              monotone_constraints=None, n_estimators=100,\n",
       "                              n_jobs=0, num_parallel_tree=1,\n",
       "                              objective='reg:squarederror', random_state=0,\n",
       "                              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                              subsample=1, tree_method=None,\n",
       "                              validate_parameters=False, verbosity=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X['LotFrontage'].fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor())]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Fit the pipeline\n",
    "xgb_pipeline.fit(X.to_dict('records'), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validating your XGBoost model\n",
    "In this exercise, you'll go one step further by using the pipeline you've created to preprocess and cross-validate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold RMSE:  27840.01103862237\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X['LotFrontage'].fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:squarederror\"))]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Cross-validate the model\n",
    "cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict('records'), y, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print the 10-fold RMSE\n",
    "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kidney disease case study I: Categorical Imputer\n",
    "You'll now continue your exploration of using pipelines with a dataset that requires significantly more wrangling. The chronic kidney disease dataset contains both categorical and numeric features, but contains lots of missing values. The goal here is to predict who has chronic kidney disease given various blood indicators as features.\n",
    "\n",
    "As Sergey mentioned in the video, you'll be introduced to a new library, [sklearn_pandas](https://github.com/scikit-learn-contrib/sklearn-pandas), that allows you to chain many more processing steps inside of a pipeline than are currently supported in scikit-learn. Specifically, you'll be able to impute missing categorical values directly using the Categorical_Imputer() class in sklearn_pandas, and the DataFrameMapper() class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame.\n",
    "\n",
    "We've also created a transformer called a Dictifier that encapsulates converting a DataFrame using .to_dict(\"records\") without you having to do it explicitly (and so that it works in a pipeline). Finally, we've also provided the list of feature names in kidney_feature_names, the target name in kidney_target_name, the features in X, and the target in y.\n",
    "\n",
    "In this exercise, your task is to apply the CategoricalImputer to impute all of the categorical columns in the dataset. You can refer to how the numeric imputation mapper was created as a template. Notice the keyword arguments input_df=True and df_out=True? This is so that you can work with DataFrames instead of arrays. By default, the transformers are passed a numpy array of the selected columns as input, and as a result, the output of the DataFrame mapper is also an array. Scikit-learn transformers have historically been designed to work with numpy arrays, not pandas DataFrames, even though their basic indexing interfaces are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chronic = pd.read_csv('datasets/chronic_kidney_disease.csv', header=None, na_values='?')\n",
    "X = df_chronic.iloc[:, :-1]\n",
    "y = df_chronic.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       9\n",
      "1      12\n",
      "2      47\n",
      "3      46\n",
      "4      49\n",
      "5     152\n",
      "6      65\n",
      "7       4\n",
      "8       4\n",
      "9      44\n",
      "10     19\n",
      "11     17\n",
      "12     87\n",
      "13     88\n",
      "14     52\n",
      "15     71\n",
      "16    106\n",
      "17    131\n",
      "18      2\n",
      "19      2\n",
      "20      2\n",
      "21      1\n",
      "22      1\n",
      "23      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn_pandas import CategoricalImputer\n",
    "from  sklearn.impute import SimpleImputer\n",
    "\n",
    "# Check number of nulls in each feature column\n",
    "nulls_per_column = X.isnull().sum()\n",
    "print(nulls_per_column)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_feature_mask = X.dtypes == object\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "                                            [([numeric_feature],SimpleImputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
    "                                            input_df=True,\n",
    "                                            df_out=True\n",
    "                                           )\n",
    "\n",
    "# Apply categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "                                                [(category_feature, CategoricalImputer()) for category_feature in categorical_columns],\n",
    "                                                input_df=True,\n",
    "                                                df_out=True\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kidney disease case study II: Feature Union\n",
    "Having separately imputed numeric as well as categorical columns, your task is now to use scikit-learn's FeatureUnion to concatenate their results, which are contained in two separate transformer objects - numeric_imputation_mapper, and categorical_imputation_mapper, respectively.\n",
    "\n",
    "You may have already encountered FeatureUnion in Machine Learning with the Experts: School Budgets. Just like with pipelines, you have to pass it a list of (string, transformer) tuples, where the first half of each tuple is the name of the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "                                         \n",
    "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
    "                                           (\"cat_mapper\", categorical_imputation_mapper)\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kidney disease case study III: Full pipeline\n",
    "It's time to piece together all of the transforms along with an XGBClassifier to build the full pipeline!\n",
    "\n",
    "Besides the numeric_categorical_union that you created in the previous exercise, there are two other transforms needed: the Dictifier() transform which we created for you, and the DictVectorizer().\n",
    "\n",
    "After creating the pipeline, your task is to cross-validate it to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chronic = pd.read_csv('datasets/chronic_kidney_disease.csv', header=None, na_values='?')\n",
    "X = df_chronic.iloc[:, :-1]\n",
    "y = df_chronic.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create full pipeline\n",
    "# pipeline = Pipeline([\n",
    "#                      (\"featureunion\", numeric_categorical_union),\n",
    "#                       (\"dictifier\", categorical_imputation_mapper),\n",
    "#                      (\"vectorizer\", DictVectorizer(sort=False)),\n",
    "#                      (\"clf\", xgb.XGBClassifier(max_depth=3))\n",
    "#                     ])\n",
    "\n",
    "# # Perform cross-validation\n",
    "# cross_val_scores = cross_val_score(pipeline, X, y, scoring=\"roc_auc\", cv=3)\n",
    "\n",
    "# # Print avg. AUC\n",
    "# print(\"3-fold AUC: \", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together\n",
    "Alright, it's time to bring together everything you've learned so far! In this final exercise of the course, you will combine your work from the previous exercises into one end-to-end XGBoost pipeline to really cement your understanding of preprocessing and pipelines in XGBoost.\n",
    "\n",
    "Your work from the previous 3 exercises, where you preprocessed the data and set up your pipeline, has been pre-loaded. Your job is to perform a randomized search and identify the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the parameter grid\n",
    "# gbm_param_grid = {\n",
    "#     'clf__learning_rate': np.arange(0.05, 1, 0.05),\n",
    "#     'clf__max_depth': np.arange(3, 10, 1),\n",
    "#     'clf__n_estimators': np.arange(50, 200, 50)\n",
    "# }\n",
    "\n",
    "# # Perform RandomizedSearchCV\n",
    "# randomized_roc_auc = RandomizedSearchCV(param_distributions=gbm_param_grid, estimator=pipeline, cv=2, n_iter=2, verbose=1, scoring='roc_auc')\n",
    "\n",
    "# # Fit the estimator\n",
    "# randomized_roc_auc.fit(X, y)\n",
    "\n",
    "# # Compute metrics\n",
    "# print(randomized_roc_auc.best_score_)\n",
    "# print(randomized_roc_auc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
